# Core ML/Training packages for LLaMA finetuning
torch>=2.0.0
transformers>=4.36.0
datasets>=2.14.0
accelerate>=0.25.0
bitsandbytes>=0.41.0
peft>=0.7.0
trl>=0.7.0

# Optional: 2x faster training
# unsloth @ git+https://github.com/unslothai/unsloth.git

# Utilities
pandas>=2.0.0
numpy>=1.24.0
tqdm>=4.65.0
wandb>=0.16.0  # For experiment tracking
